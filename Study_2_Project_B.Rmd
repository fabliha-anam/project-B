---
title: "431 Project B Study 2"
author: "Fabliha Anam and Sam Waddell"
date: "`r Sys.Date()`"
linkcolor: blue
output:
  rmdformats::readthedown:
    highlight: kate
    number_sections: true
    code_folding: show
    df_print: paged
---

# Setup and Data Ingest 

## Initial Setup and Package Loads in R 

```{r initial_setup, message = FALSE, warning = FALSE}
library(knitr)
library(rmdformats)
library(rmarkdown)
library(GGally)
library(patchwork)
library(car)
library(equatiomatic)
library(janitor)
library(magrittr)
library(mosaic)
library(naniar)
library(simputation)
library(broom)
library(nhanesA)
library(tidyverse) 

## Global options

options(max.print="75")
opts_chunk$set(comment=NA)
opts_knit$set(width=75)

theme_set(theme_bw())
options(dplyr.summarise.inform = FALSE)
```

## Loading the Raw Data into R 

Here, we load the data using `read_csv` and then convert all `character` variables to `factors` in R, and then change our identifying code: `subj_id` back to a character variable.

```{r data_load, message = FALSE}

demo_raw <- nhanes('DEMO_J') %>% tibble()
dr1tot_raw <- nhanes('DR1TOT_J') %>% tibble()
dbq_raw <- nhanes('DBQ_J') %>% tibble()
hiq_raw <- nhanes('HIQ_J') %>% tibble()
mcq_raw <- nhanes('MCQ_J') %>% tibble()


```


From demographics, we want the following variables:

- SEQN
- RIDSTATR
- RIDRETH3
- INDFMPIR

```{r demo1}
demo1 <- demo_raw %>%
  select(SEQN, RIDSTATR, RIDRETH3, INDFMPIR) %>%
  filter(complete.cases(.))

dim(demo1)

```

From DR1TOT, we want: 

- SEQN
- DR1TKCAL

```{r dr1tot1}
dr1tot1 <- dr1tot_raw %>%
  select(SEQN, DR1TKCAL) %>%
  filter(complete.cases(.))

dim(dr1tot1)
```

From DBQ, we want:

- SEQN 
- DBQ700

```{r dbq1}
dbq1 <- dbq_raw %>% 
  select(SEQN, DBQ700) %>%
  filter(complete.cases(.))

dim(dbq1)
```

From MCQ, we want:

- SEQN
- MCQ080

```{r mcq1}
mcq1 <- mcq_raw %>%
  select(SEQN, MCQ080) %>%
  filter(complete.cases(.))

dim(mcq1)
```

From HIQ, we want:

- SEQN
- HIQ011

```{r hiq1}
hiq1 <- hiq_raw %>%
  select(SEQN, HIQ011) %>% filter(complete.cases(.))

dim(hiq1)
```


# Merging the Data

```{r New}
new <- left_join(demo1, dr1tot1, by = "SEQN")

dim(new)

```

```{r new2}
new2 <- left_join(new, dbq1, by = "SEQN")

dim(new2)
```

```{r new3}
new3 <- left_join(new2, mcq1, by = "SEQN")

dim(new3)
```

```{r new4}
new4 <- left_join(new3, hiq1, by = "SEQN")

dim(new4)
```

```{r project_b_raw}
study2_raw <- new4 %>% filter(complete.cases(.))

dim(study2_raw)
```


# Cleaning the Data

## The Raw Data

The `hbp_study` data set includes 12 variables and 999 adult subjects. For each subject, we have gathered

- baseline information on their `age`, and their `sex`, 
- whether or not they have a `diabetes` diagnosis, 
- the socio-economic status of their neighborhood of residence (`nses`), 
- their body-mass index (`bmi1`) and systolic blood pressure (`sbp1`), 
- their `insurance` type, `tobacco` use history, and 
- whether or not they have a prescription for a `statin`, or for a `diuretic`. 
- Eighteen months later, we gathered a new systolic blood pressure (`sbp2`) for each subject.

```{r hbp_study_data_in_the_raw}
glimpse(hbp_study)
```

**Note**: If you have more than 20 variables in your initial (raw) data set, prune it down to 20 as the first step before showing us the results of `glimpse` for your data.

This tibble describes twelve variables, including:

- a character variable called `subj_id` not to be used in our model except for identification of subjects,
- our outcome (`sbp2`) and our key predictor (`sbp1`) that describe systolic blood pressure at two different times.
- seven categorical candidate predictors, specifically `sex`, `diabetes`, `nses`, `insurance`, `tobacco`, `statin`, and `diuretic`, each specified here in R as either a factor or a 1/0 numeric variable (`statin` and `diuretic`),
- three quantitative candidate predictors, specifically `age`, `bmi1` and `sbp1`. 

## Which variables should be included in the tidy data set?

In fitting my models, I actually plan only to use five predictors: `sbp1`, `age`, `bmi1`, `diabetes` and `tobacco` to model my outcome: `sbp2`. Even though I'm not planning to use all of these predictors in my models, I'm going to build a tidy data set including all of them anyway, so I can demonstrate solutions to some problems you might have. 

When you build your tidy data set in the next section, restrict it to the variables (outcomes, predictors and `subj_id`) that you will actually use in your modeling. 

In building our tidy version of these data, we must:

- deal with the ordering of levels in the multi-categorical variables `nses`, `insurance` and `tobacco`,
- change the name of `nses` to something more helpful - I'll use `nbhd_ses` as the new name^[Admittedly, that's not much better.].

## Checking our Outcome and Key Predictor

```{r}
df_stats(~ sbp2 + sbp1, data = hbp_study)
```

We have no missing values in our outcome or our key predictor, and each of the values look plausible, so we'll move on.

## Checking the Quantitative Predictors

Besides `sbp1` we have two other quantitative predictor candidates, `age` and `bmi1`.

```{r}
df_stats(~ age + bmi1, data = hbp_study)
```

We know that all subjects in these data had to be between 33 and 83 years of age in order to be included, so we're happy to see that they are. We have five missing values (appropriately specified with NA) and no implausible values in our BMI values (I would use 16-80 as a plausible range of BMI values for adults.) Things look OK for now, as we'll deal with the missing values last.

## Checking the Categorical Variables

For categorical variables, it's always worth it to check to see whether the existing orders of the factor levels match the inherent order of the information, as well as whether there are any levels which we might want to collapse due to insufficient data, and whether there are any missing values.

### `nses`: home neighborhood's socio-economic status

```{r levels_of_nses}
hbp_study %>% tabyl(nses)
```

- The order of `nses`, instead of the alphabetical ("High", "Low", "Middle", "Very Low"), should go from "Very Low" to "Low" to "Middle" to "High", or perhaps its reverse.
- Let's fix that using the `fct_relevel` function from the `forcats` package, which is part of the `tidyverse`. While we're at it, we'll rename the variable `nbhd_ses` which is more helpful to me.
- Then we'll see how many subjects fall in each category.

```{r relevel_nses}
hbp_study <- hbp_study %>%
  rename(nbhd_ses = nses) %>%
  mutate(nbhd_ses = fct_relevel(nbhd_ses, "Very Low", "Low", 
                            "Middle", "High"))
hbp_study %>% tabyl(nbhd_ses)
```

We have 8 missing values of `nbhd_ses`. We'll deal with that later.

### `tobacco`: tobacco use history

```{r levels_of_tobacco}
hbp_study %>% tabyl(tobacco)
```

- For `tobacco`, instead of ("current", "never", "quit"), we want a new order: ("never", "quit", "current").

```{r relevel_tobacco}
hbp_study <- hbp_study %>%
  mutate(tobacco = fct_relevel(tobacco, "never", "quit", 
                            "current"))
hbp_study %>% count(tobacco)
```

We have 23 missing values of `tobacco`. Again, we'll deal with that later.

### `insurance`: primary insurance type

```{r levels_insurance}
hbp_study %>% tabyl(insurance)
```

- For `insurance`, we'll change the order to ("Medicare", "Private", "Medicaid", "Uninsured")

```{r relevel_insurance}
hbp_study <- hbp_study %>%
  mutate(insurance = fct_relevel(insurance, "Medicare", 
                                 "Private", "Medicaid", 
                                 "Uninsured"))
hbp_study %>% tabyl(insurance)
```

Note that any levels left out of a `fct_relevel` statement get included in their current order, after whatever levels have been specified.

### What about the subjects?

It is important to make sure that we have a unique (distinct) code (here, `subj_id`) for each row in the raw data set.

```{r}
nrow(hbp_study)
n_distinct(hbp_study %>% select(subj_id))
```

OK, that's fine.

## Dealing with Missingness

Note that you will need to ensure that any *missing* values are appropriately specified using `NA`. 

- In this data set, we're all set on that issue. 
    + There are missing data in `nses` (8 NA), `bmi1` (5 NA) and `tobacco` (23 NA).
- **Missing Outcomes**. In building your tidy data set, delete any subjects with missing values of your outcome variable. I'd also probably drop any subjects missing the key predictor, too.
    + If we needed to delete the rows with missing values of an outcome, I would use code of the form `data_fixed <- data_original %>% filter(complete.cases(outcomevariablename))` to accomplish that.
    + The elements (`sbp1` and `sbp2`) that form our outcome and key predictor have no missing values, though, so we'll be OK in that regard.

In building the tidy data set, leave all missing values for candidate predictors as `NA`. 

### Assume MCAR and Build Tibble of Complete Cases

For your project, I expect a complete case analysis, where you drop all observations with missing data from your data set before creating your codebook. Let's do that here.

### Identifying Missing Data

There are 23 subjects missing `tobacco`, 8 missing `nbhd_ses` and 5 missing `bmi1`.

```{r}
miss_var_summary(hbp_study)
```

No subject is missing more than one variable, as we can tell from the table below, sorted by `n_miss`.

```{r}
miss_case_summary(hbp_study)
```

So we'll lose 23 + 8 + 5 = 36 observations from our sample of 999 in dropping all cases with missing values, leaving us with a complete cases tibble of 963 rows.

### The Complete Cases Tibble

My complete case data set would then be something like this:

```{r hbp_cc}
hbp_cc <- hbp_study %>%
  select(subj_id, sbp2, sbp1, age, sex, 
         diabetes, nbhd_ses, bmi1, insurance, 
         tobacco, statin, diuretic) %>%
  filter(complete.cases(.))

hbp_cc
```

That's fine for Project B.

### What if we instead did imputation?

As an alternative (NOT expected in any way for Project B), we could use the `simputation` package to impute missing values of `tobacco`, `bmi1` and `nbhd_ses`. For each of these, we need to specify the approach we will use to do the imputation, and the variables we plan to use as predictors in our imputation model (the variables we plan to use to help predict the missing values). Some of these choices will be a little arbitrary, but I'm mostly demonstrating options here.

Variable | NAs | Class | Imputation Approach | Imputation Model Predictors
-------: | ---: | -----------: | -------------------------- | -------------------
`nbhd_ses` | `r sum(is.na(hbp_study$nbhd_ses))` | `r class(hbp_study$nbhd_ses)` | CART (decision tree) | `age`, `sex`, `insurance`
`tobacco` | `r sum(is.na(hbp_study$tobacco))` | `r class(hbp_study$tobacco)` | CART (decision tree) | `age`, `sex`, `insurance`, `nbhd_ses`
`bmi1` | `r sum(is.na(hbp_study$bmi1))` | `r class(hbp_study$bmi1)` | Robust Linear Model | `age`, `sex`, `diabetes`, `sbp1`

Here's the actual set of imputation commands, to create an imputed data set named `hbp_imputed`.

```{r}
hbp_imputed <- hbp_study %>%
  impute_cart(nbhd_ses ~ age + sex + insurance) %>%
  impute_cart(tobacco ~ age + sex + insurance + nbhd_ses) %>%
  impute_rlm(bmi1 ~ age + sex + diabetes + sbp1)

summary(hbp_imputed)
```

Note that I imputed `tobacco` after `nbhd_ses` largely because I wanted to use the `nbhd_ses` results to aid in my imputation of `tobacco`.

# My Research Question

Here you should provide background information on the study, and the subjects, so that we understand what you're talking about in your research question. I'll skip that in the demo, because I've done it already in introducing the data set, but you'll need that here.

A natural research question here would be something like:

> How effectively can we predict systolic BP 18 months after baseline using baseline systolic BP, and is the quality of prediction meaningfully improved when I adjust for four other predictors (baseline age, body-mass index, diabetes diagnosis and tobacco use) in the `hbp_study` data? 

Please don't feel obliged to follow this format precisely in stating your question.

# My Final Variables

## The Codebook

We'll use the `hbp_cc` data for the rest of this demonstration. The 12 variables in our tidy data set for this demonstration are as follows. 

Variable      | Type  | Description / Levels
---------: | :-------------: | --------------------------------------------
`subj_id`   | Character  | subject code (A001-A999)
`sbp2`      | Quantitative | **outcome** variable, SBP after 18 months, in mm Hg
`sbp1`      | Quantitative | **key predictor** baseline SBP (systolic blood pressure), in mm Hg
`age`       | Quantitative | age of subject at baseline, in years
`sex`       | Binary | Male or Female
`diabetes`  | Binary | Does subject have a diabetes diagnosis: No or Yes
`nbhd_ses`  | 4 level Cat. | Socio-economic status of subject's home neighborhood: Very Low, Low, Middle and High
`bmi1`      | Quantitative | subject's body-mass index at baseline
`insurance` | 4 level Cat. | subject's insurance status at baseline: Medicare, Private, Medicaid, Uninsured
`tobacco`   | 3 level Cat. | subject's tobacco use at baseline: never, quit (former), current
`statin`    | Binary | 1 = statin prescription at baseline, else 0
`diuretic`  | Binary | 1 = diuretic prescription at baseline, else 0

**Note**: I've demonstrated this task for a larger set of predictors than I actually intend to use. In fitting my models, I actually plan only to use five predictors: `sbp1`, `age`, `bmi1`, `diabetes` and `tobacco` to model my outcome: `sbp2`.

## Numerical Data Description

Here, I'll focus only on the variables I actually will use in the analyses.

```{r}
hbp_analytic <- hbp_cc %>%
  select(subj_id, sbp2, sbp1, age, bmi1, diabetes, tobacco) 

hbp_analytic %>% 
  select(-subj_id) %$%
  Hmisc::describe(.)
```

We should (and do) see no missing or implausible values here, and our categorical variables are treated as factors with a rational ordering for the levels.

# Partitioning the Data

Here, we will obtain a training sample with a randomly selected 80% of the data (after imputing), and have the remaining 20% in a test sample, properly labeled, and using `set.seed` so that the results can be replicated later. 

I will call the training sample `hbp_training` and the test sample `hbp_test`. 

  - The `slice_sample` function will sample the specified proportion of the data.
  - Your training sample should contain a randomly selected 70-80% of your data. 
  - The `anti_join` function returns all rows in the first data frame (here specified as `hbp_analytic`) that are not in the second data frame (here specified as `hbp_training`) as assessed by the row-specific identification code (here `subj_id`)).

```{r splitting_samples}
set.seed(431) # set your own seed, don't use this one

hbp_training <- hbp_analytic %>% 
  slice_sample(., prop = .70)
hbp_test <- anti_join(hbp_analytic, hbp_training, by = "subj_id")

dim(hbp_analytic) # number of rows and columns in hbp_analytic
dim(hbp_training) # check to be sure we have 70% of hbp_analytic here
dim(hbp_test) # check to be sure we have the rest of hbp_analytic here
```

Since 674 + 289 = 963, we should be OK. 

# Transforming the Outcome

## Visualizing the Outcome Distribution

I see at least three potential graphs to use to describe the distribution of our outcome variable, `sbp2`. Again, remember we're using only the **training** sample here.

- A boxplot, probably accompanied by a violin plot to show the shape of the distribution more honestly.
- A histogram, which could perhaps be presented as a density plot with a Normal distribution superimposed.
- A Normal Q-Q plot to directly assess Normality.

I expect you to show at least two of these three, but I will display all three here. Should we see substantial skew in the outcome data, we will want to consider an appropriate transformation, and then display the results of that transformation, as well.

**WARNING**: Please note that I am deliberately showing you plots that are less finished than I hope you will provide. 

  - The coloring is dull or non-existent.
  - The theme is the default gray and white grid that lots of people dislike.
  - There are no meaningful titles or subtitles.
  - The axis labels select the default settings, and use incomprehensible variable names.
  - The coordinates aren't flipped when that might be appropriate.
  - I expect a much nicer presentation in your final work. Use the class slides and Lab answer sketches as a model for better plotting.

```{r}
viz1 <- ggplot(hbp_training, aes(x = "", y = sbp2)) +
  geom_violin() +
  geom_boxplot(width = 0.25)

viz2 <- ggplot(hbp_training, aes(x = sbp2)) +
  geom_histogram(bins = 30, col = "white")

viz3 <- ggplot(hbp_training, aes(sample = sbp2)) +
  geom_qq() + geom_qq_line()

viz1 + viz2 + viz3 +
  plot_annotation(title = "Less-Than-Great Plots of My Outcome's Distribution",
                  subtitle = "complete with a rotten title, default axis labels and bad captions")
```

Later, we'll augment this initial look at the outcome data with a Box-Cox plot to suggest a potential transformation. Should you decide to make such a transformation, remember to return here to plot the results for your new and transformed outcome.

## Numerical Summary of the Outcome

Assuming you plan no transformation of the outcome (and in our case, I am happy that the outcome data appear reasonably well-modeled by the Normal distribution) then you should just summarize the training data, with your favorite tool for that task. That might be:

- `favstats` from the `mosaic` package, as shown below, or
- `describe` from the `Hmisc` package, or 
- something else, I guess. 

But show **ONE** of these choices, and not all of them. Make a decision and go with it!

```{r}
favstats(~ sbp2, data = hbp_training)
```

## Numerical Summaries of the Predictors

We also need an appropriate set of numerical summaries of each predictor variable, in the training data. I see at least two potential options here:

1. Use `inspect` from the `mosaic` package to describe the predictors of interest briefly.
3. Use `describe` from the `Hmisc` package for a more detailed description of the entire data set.

Again, **DO NOT** do all of these. Pick one that works for you. I'm just demonstrating possible choices here.

### Using the `inspect` function from `mosaic`

The `inspect` function provides a way to get results like `favstats`, but for an entire data frame.

```{r}
hbp_training %>% select(-subj_id, -sbp2) %>% 
  mosaic::inspect()
```

Next, we will build and interpret a scatterplot matrix to describe the associations (both numerically and graphically) between the outcome and all predictors. 

- We'll also use a Box-Cox plot to investigate whether a transformation of our outcome is suggested, and
- describe what a correlation matrix suggests about collinearity between candidate predictors.

## Scatterplot Matrix

Here, we will build a scatterplot matrix (or two) to show the relationship between our outcome and the predictors. I'll demonstrate the use of `ggpairs` from the `GGally` package.

- If you have more than five predictors (as we do in our case) you should build two scatterplot matrices, each ending with the outcome. Anything more than one outcome and five predictors becomes unreadable in Professor Love's view.
- If you have a multi-categorical predictor with more than four categories, that predictor will be very difficult to see and explore in the scatterplot matrix produced.

```{r, message = FALSE}
hbp_training %>% 
  select(sbp1, age, bmi1, diabetes, tobacco, sbp2) %>% 
  ggpairs(., title = "Scatterplot Matrix",
          lower = list(combo = wrap("facethist", bins = 20)))
```

At the end of this section, you should provide some discussion of the distribution of any key predictors, and their relationship to the outcome (all of that is provided in the bottom row if you place the outcome last, as you should, in selecting variables for the plot.)

**HINT**: For categorical variables, your efforts in this regard to summarize the relationships you see may be challenging. Your comments would be aided by the judicious use of numerical summaries. For example, suppose you want to study the relationship between tobacco use and `sbp2`, then you probably want to run and discuss the following results, in addition to the scatterplot matrix above.

```{r}
mosaic::favstats(sbp2 ~ tobacco, data = hbp_training)
```

## Collinearity Checking

Next, we'll take a brief look at potential collinearity. Remember that we want to see strong correlations between our **outcome** and the predictors, but relatively modest correlations between the predictors.

None of the numeric candidate predictors show any substantial correlation with each other. The largest Pearson correlation (in absolute value) between predictors is (-0.239) for `age` and `bmi1`, and that's not strong. If we did see signs of meaningful collinearity, we might rethink our selected set of predictors.

I'll recommend later that you run a generalized VIF (variance inflation factor) calculation^[As we'll see in that setting, none of the generalized variance inflation factors will exceed 1.1, let alone the 5 or so that would cause us to be seriously concerned about collinearity.] after fitting your kitchen sink model just to see if anything pops up (in my case, it won't.) 

## `boxCox` function to assess need for transformation of our outcome

To use the `boxCox` approach here, we need to ensure that the distribution of our outcome, `sbp2`, includes strictly positive values. We can see from our numerical summary earlier that the minimum `sbp2` in our `hbp_training` sample is 90, so we're OK.

- Note that I am restricting myself here to the five predictors I actually intend to use in building models.
- Although we're generally using a 90% confidence interval in this project, we won't worry about that issue in the `boxCox` plot, and instead just look at the point estimate from `powerTransform`. 
- These commands (`boxCox` and `powerTransform`) come from the `car` package.

```{r boxCox_plot}
model_temp <- lm(sbp2 ~ sbp1 + age + bmi1 + diabetes + tobacco,
                 data = hbp_training)

boxCox(model_temp)

powerTransform(model_temp)
```

The estimated power transformation is about 0.5, which looks like a square root transformation of `sbp2` is useful. Given that I'm using another measure of `sbp`, specifically, `sbp1` to predict `sbp2`, perhaps I want to transform that, too?

```{r}
p1 <- ggplot(hbp_training, aes(x = sbp1, y = sqrt(sbp2))) +
  geom_point() +
  geom_smooth(method = "loess", formula = y ~ x, se = FALSE) + 
  geom_smooth(method = "lm", col = "red", formula = y ~ x, se = FALSE) +
  labs(title = "SQRT(sbp2) vs. SBP1")

p2 <- ggplot(hbp_training, aes(x = sqrt(sbp1), y = sqrt(sbp2))) +
  geom_point() +
  geom_smooth(method = "loess", formula = y ~ x, se = FALSE) + 
  geom_smooth(method = "lm", col = "red", formula = y ~ x, se = FALSE) + 
  labs(title = "SQRT(sbp2) vs. SQRT(sbp1)")

p1 + p2
```

I don't see an especially large difference between these two plots. It is up to you to decide whether a transformation suggested by `boxCox` should be applied to your data.

- For the purposes of this project, you should stick to transformations of strictly positive outcomes, and to the square root (power = 0.5), square (power = 2), logarithm (power = 0) and inverse (power = -1) transformations. Don't make the transformation without being able to interpret the result well.
- If you do decide to include a transformation of your outcome in fitting models, be sure to back-transform any predictions you make at the end of the study so that we can understand the prediction error results.
- If your outcome data are substantially multimodal, I wouldn't treat the `boxCox` results as meaningful. 

I'm going to use the square root transformation for both my outcome and for the key predictor, but I don't think it makes a big difference, mostly so that I can show you how to back-transform later.

# The Big Model

We will specify a "kitchen sink" linear regression model to describe the relationship between our outcome (potentially after transformation) and the main effects of each of our predictors. We'll need to:

- We'll assess the overall effectiveness, within your training sample, of your model, by specifying and interpreting the R^2^, adjusted R^2^ (especially in light of our collinearity conclusions, below), the residual standard error, and the ANOVA F test. 
- We'll need to specify the size, magnitude and meaning of all coefficients, and identify appropriate conclusions regarding effect sizes with 90% confidence intervals.
- Finally, we'll assess whether collinearity in the kitchen sink model has a meaningful impact, and describe how we know that.

## Fitting/Summarizing the Kitchen Sink model

Our "kitchen sink" or "big" model predicts the square root of `sbp2` using the predictors (square root of `sbp1`), `age`, `bmi1`, `diabetes` and `tobacco`.

```{r kitchen_sink}
model_big <- lm(sqrt(sbp2) ~ sqrt(sbp1) + age + bmi1 + diabetes + tobacco, 
                data = hbp_training)
```

```{r}
summary(model_big)
```


## Effect Sizes: Coefficient Estimates

Specify the size, magnitude and meaning of all coefficients, and identify appropriate conclusions regarding effect sizes with 90% confidence intervals.

```{r}
tidy(model_big, conf.int = TRUE, conf.level = 0.90) %>% 
  select(term, estimate, std.error, conf.low, conf.high, p.value) %>% 
  kable(dig = 4)
```

I wanted to get at least two significant figures in my coefficient and standard error estimates for all of the predictors in this model, so that's why I had to go out to 4 decimal places.

## Describing the Equation

We could use the `equatiomatic` package to present the model. Note the use of **results = 'asis'** in the chunk setup in order to get the results to show up in my HTML. Be sure to use enough digits so that you have at least a couple of significant figures in each coefficient, please.

```{r, results = 'asis'}
extract_eq(model_big, use_coefs = TRUE, coef_digits = 4,
           terms_per_line = 3, wrap = TRUE, ital_vars = TRUE)
```

This model implies for the key predictor that:

- for every increase of one point in the square root of `sbp1`, we anticipate an increase in the outcome (square root of sbp2) of 0.37 mm Hg (90% confidence interval: 0.31, 0.43). If we had two subjects with the same values of all other variables, but A had a baseline square root of SBP of 12 (so an SBP at baseline of 144) and B had a baseline square root of SBP of 11 (so an SBP at baseline of 121) then if all other variables are kept at the same value, our model predicts that the square root of subject A's SBP at 18 months will be 0.37 points higher (90% CI: 0.31, 0.43) than that of subject B.

You should provide a (briefer) description of the meaning (especially the direction) of the other coefficients in your model being sure only to interpret the coefficients as having meaning *holding all other predictors constant*, but I'll skip that in the demo.

# The Smaller Model

Here, we will build a second linear regression model using a subset of our "kitchen sink" model predictors, chosen to maximize predictive value within our training sample. 

- We'll specify the method you used to obtain this new model. (Backwards stepwise elimination is appealing but not great. It's perfectly fine to just include the key predictor as the subset for this new model.) 

## Backwards Stepwise Elimination

```{r stepwise_bw_model}
step(model_big)
```

The backwards selection stepwise approach suggests a model with `sqrt(sbp1)` and age and tobacco, but not `bmi1` or `diabetes`.

1. If stepwise regression retains the kitchen sink model or if you don't want to use stepwise regression, develop an alternate model by selecting a subset of the Big model predictors (including the key predictor) on your own. A simple regression on the key predictor is a perfectly reasonable choice.
2. If stepwise regression throws out the key predictor in your kitchen sink model, then I suggest not using stepwise regression.
3. We will learn several other methods for variable selection in 432. If you want to use one of them (for instance, a C_p_ plot) here, that's OK, but I will hold you to high expectations for getting that done correctly, and it's worth remembering that none of them work very well at identifying the "best" model.

## Fitting the "small" model

```{r fit_mod_small}
model_small <- lm(sqrt(sbp2) ~ sqrt(sbp1) + age + tobacco, data = hbp_training)

summary(model_small)
```

## Effect Sizes: Coefficient Estimates

Here, we need to specify the size, magnitude and meaning of all coefficients, and identify appropriate conclusions regarding effect sizes with 90% confidence intervals.

```{r}
tidy(model_small, conf.int = TRUE, conf.level = 0.90) %>% 
  select(term, estimate, std.error, conf.low, conf.high, p.value) %>% 
  kable(dig = 4)
```

## Small Model Regression Equation

```{r, results = 'asis'}
extract_eq(model_small, use_coefs = TRUE, coef_digits = 4,
           terms_per_line = 3, wrap = TRUE, ital_vars = TRUE)
```

I'll skip the necessary English sentences here in the demo that explain the meaning of the estimates in our model. 

# In-Sample Comparison

## Quality of Fit

We will compare the small model to the big model in our training sample using adjusted R^2^, the residual standard error, AIC and BIC. We'll be a little bit slick here. 

- First, we'll use `glance` to build a tibble of key results for the kitchen sink model, and append to that a description of the model. We'll toss that in a temporary tibble called `temp_a`.
- Next we do the same for the smaller model, and put that in `temp_b`.
- Finally, we put the temp files together into a new tibble, called `training_comp`, and examine that.

```{r}
temp_a <- glance(model_big) %>% 
  select(-logLik, -deviance) %>%
  round(digits = 3) %>%
  mutate(modelname = "big")

temp_b <- glance(model_small) %>%
  select(-logLik, -deviance) %>%
  round(digits = 3) %>%
  mutate(modelname = "small")

training_comp <- bind_rows(temp_a, temp_b) %>%
  select(modelname, nobs, df, AIC, BIC, everything())
```

```{r}
training_comp
```

It looks like the smaller model with 3 predictors: `sqrt(sbp1)`, `age` and `tobacco`, performs slightly better in the training sample.

- The AIC and BIC for the smaller model are each a little smaller than for the big model.
- The adjusted R^2^ and residual standard deviation (`sigma`) is essentially identical in the two models.

## Assessing Assumptions

Here, we should run a set of residual plots for each model. If you want to impress me a little, you'll use the `ggplot` versions I introduced in the slides for Classes 22-24. Otherwise, it's perfectly fine just to show the plots available in base R.

### Residual Plots for the Big Model

```{r}
par(mfrow = c(2,2)); plot(model_big); par(mfrow = c(1,1))
```

I see no serious problems with the assumptions of linearity, Normality and constant variance, nor do I see any highly influential points in our big model.

### Residual Plots for the Small Model

```{r}
par(mfrow = c(2,2)); plot(model_small); par(mfrow = c(1,1))
```

I see no serious problems with the assumptions of linearity, Normality and constant variance, nor do I see any highly influential points in our small model.

### Does collinearity have a meaningful impact?

If we fit models with multiple predictors, then we might want to assess the potential impact of collinearity.

```{r}
car::vif(model_big)
```

We'd need to see a generalized variance inflation factor above 5 for collinearity to be a meaningful concern, so we should be fine in our big model. Our small model also has multiple predictors, but it cannot be an issue, since it's just a subset of our big model, which didn't have a collinearity problem.

## Comparing the Models

Based on the training sample, my conclusions so far is to support the smaller model. It has (slightly) better performance on the fit quality measures, and each model shows no serious problems with regression assumptions.

# Model Validation

Now, we will use our two regression models to predict the value of our outcome using the predictor values  in the test sample. 

- We may need to back-transform the predictions to the original units if we wind up fitting a model to a transformed outcome. 
- We'll definitely need to compare the two models in terms of mean squared prediction error and mean absolute prediction error in a Table, which I will definitely want to see in your portfolio. 
- We'll have to specify which model appears better at out-of-sample prediction according to these comparisons, and how we know that.

## Calculating Prediction Errors

### Big Model: Back-Transformation and Calculating Fits/Residuals

We'll use the `augment` function from the `broom` package to help us here, and create `sbp2_fit` to hold the fitted values on the original `sbp2` scale after back-transformation (by squaring the predictions on the square root scale) and then `sbp2_res` to hold the residuals (prediction errors) we observe using the big model on the `hbp_test` data.

```{r}
aug_big <- augment(model_big, newdata = hbp_test) %>% 
  mutate(mod_name = "big",
         sbp2_fit = .fitted^2,
         sbp2_res = sbp2 - sbp2_fit) %>%
  select(subj_id, mod_name, sbp2, sbp2_fit, sbp2_res, everything())

head(aug_big,3)
```

### Small Model: Back-Transformation and Calculating Fits/Residuals

We'll do the same thing, but using the small model in the `hbp_test` data.

```{r}
aug_small <- augment(model_small, newdata = hbp_test) %>% 
  mutate(mod_name = "small",
         sbp2_fit = .fitted^2,
         sbp2_res = sbp2 - sbp2_fit) %>%
  select(subj_id, mod_name, sbp2, sbp2_fit, sbp2_res, everything())

head(aug_small,3)
```

### Combining the Results

```{r}
test_comp <- union(aug_big, aug_small) %>%
  arrange(subj_id, mod_name)

test_comp %>% head()
```

Given this `test_comp` tibble, including predictions and residuals from the kitchen sink model on our test data, we can now:

1. Visualize the prediction errors from each model.
2. Summarize those errors across each model.
3. Identify the "worst fitting" subject for each model in the test sample.

The next few subsections actually do these things.

## Visualizing the Predictions

```{r}
ggplot(test_comp, aes(x = sbp2_fit, y = sbp2)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, lty = "dashed") + 
  geom_smooth(method = "loess", col = "blue", se = FALSE, formula = y ~ x) +
  facet_wrap( ~ mod_name, labeller = "label_both") +
  labs(x = "Predicted sbp2",
       y = "Observed sbp2",
       title = "Observed vs. Predicted sbp2",
       subtitle = "Comparing Big to Small Model in Test Sample",
       caption = "Dashed line is where Observed = Predicted")
```

I'm not seeing a lot of difference between the models in terms of the adherence of the points to the dashed line. The models seem to be making fairly similar errors.

## Summarizing the Errors

Calculate the mean absolute prediction error (MAPE), the root mean squared prediction error (RMSPE) and the maximum absolute error across the predictions made by each model. 

```{r}
test_comp %>%
  group_by(mod_name) %>%
  summarize(n = n(),
            MAPE = mean(abs(sbp2_res)), 
            RMSPE = sqrt(mean(sbp2_res^2)),
            max_error = max(abs(sbp2_res)))
```

This is a table Dr. Love will **definitely** need to see during your presentation.

In this case, two of these summaries are better (smaller) for the small model (RMSPE and max_error), suggesting (gently) that it is the better choice. The MAPE is the exception. 

These models suggest an average error in predicting systolic blood pressure (using MAPE) of more than 13 mm Hg. That's not great on the scale of systolic blood pressure, I think.

### Identify the largest errors

Identify the subject(s) where that maximum prediction error was made by each model, and the observed and model-fitted values of `sbp_diff` for that subject in each case.

```{r}
temp1 <- aug_big %>%
  filter(abs(sbp2_res) == max(abs(sbp2_res)))

temp2 <- aug_small %>%
  filter(abs(sbp2_res) == max(abs(sbp2_res)))

bind_rows(temp1, temp2)
```

- In our case, the same subject (`A0513`) was most poorly fit by each model.

## Comparing the Models

I would select the smaller model here, on the basis of the similar performance in terms of the visualization of errors, and the small improvements in RMSPE and maximum prediction error.

# Discussion

## Chosen Model

I chose the Small model. You'll want to reiterate the reasons why in this subsection.

## Answering My Question

Now use the Small model to answer the research question, in a complete sentence of two.

## Next Steps

Describe an interesting next step, which might involve fitting a new model not available with your current cleaned data, or dealing with missing values differently, or obtaining new or better data, or something else. You should be able to describe why this might help.

## Reflection

Tell us what you know now that would have changed your approach to Study 2 had you known it at the start.

# Important Reminders from Dr. Love {-}

1. Remember that each subsection should include at least one complete sentence explaining what you are doing, specifying the variables you are using and how you are using them, and then conclude with at least one complete sentence of discussion of the key conclusions you draw from the current step, and a discussion of any limitations you can describe that apply to the results.

2. For heaven's sake, DO NOT use my words included in this demonstration project in your project. Rewrite everything to make it relevant to your situation. Do not repeat my instructions back to me. 

# Session Information

should be included at the end of your report.

```{r}
sessionInfo()
```
